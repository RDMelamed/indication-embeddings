import os
os.environ['MKL_THREADING_LAYER'] = 'GNU'
import model_util
model_util.tf_sess()            

import keras
from keras.models import Sequential
import tensorflow as tf
from keras.layers.embeddings import Embedding
from keras.layers.core import Dense, Dropout
from keras.layers import Reshape
from keras.models import load_model

#from keras.utils.training_utils import multi_gpu_model
#from hyperparameterConfig import Hyperparameters
from keras import optimizers
from keras.callbacks import ModelCheckpoint
from keras import backend as K
from keras.layers import Input, dot
from keras.utils import np_utils
from keras import regularizers

import time
import numpy as np
import pandas as pd
import glob
import pdb
EMBEDDING_DROPOUT =  .2
#EMBED_SIZE = 50
#NEW_DRUGS = 1270 #100

import sys
import pickle
import time
sys.path.append("../../code")
#rop.run("emb10-He",embmat,class_weight=class_weight, do_early_stopping=False, settings={'opt':'adam', 'L2':.1, 'cuda
#    ...: ':'1','batchsize':50,'numep':10})
# settings = {'opt':'adam', 'L2':.001, 'cuda':'0','batchsize':500, 'numep':20,'totdat':5000000,'valdat':3000000, 'embed_size':50}
#embed_indications.run("skips-60-480-0.6-tf/","vocab_drugs.pkl", "skips-60-480-0.6/skipdat.*",settings=settings)
def run(fnamein, vocab, skips, onlyDx=False,do_early_stopping=True,
        settings={}):

    #settingsList = ['EMBED_SIZE=' + str(EMBED_SIZE), 'onlyDx=' + str(onlyDx)] + [str(k) + "=" + '{:1.2e}'.format(v) if not type(v)==str else k + "=" + v for k,v in settings.items()]
    #settings = ['EMBED_SIZE=' + str(EMBED_SIZE), 'onlyDx=' + str(onlyDx)] + [str(k) + "=" + '{:1.2e}'.format(v) for k,v in settings.values()]
    #            'learning_rate=' + str(learning_rate), 'l2=' + str(L2),'decay=',str(decay)]
    (fname, settingsList) = model_util.make_fname(fnamein, settings)
    #fname = fnamein  + str(EMBED_SIZE) + "." + ('allelt' if not onlyDx else 'dx') + ".".join(['{:1.2e}'.format(i) if not type(i)==str else i for k, i in settings.items() if not k=='cuda']) #  + '.' + str(learning_rate) + '.' + str(L2) + "." + str(numep) + "." + str(decay)
    if not 'dropout' in settings:
        settings['dropout'] = EMBEDDING_DROPOUT
    if not 'momentum' in settings:
        settings['momentum'] = .9
    if not 'cuda' in settings:
        settings['cuda'] = "0"
    if not 'learning_rate' in settings:
        settings['learning_rate'] = .0001
    if not 'decay' in settings:
        settings['decay'] = 0
    if not 'batchsize' in settings:
        settings['batchsize'] = 500
    if not 'opt' in settings:
        settings['opt'] = 'sgd'
    if not 'totdat' in settings:
        settings['totdat'] = 100000000
    if not 'numep' in settings:
        settings['numep'] = 20
    if not 'nesterov' in settings:
        settings['nesterov'] = False
    #eltfreq = pd.read_pickle("eltfreq.p3.pkl")
    eltfreq, newdrugs = pickle.load(open(vocab,'rb'))
    '''
    eltfreq = pd.read_pickle(vocab)
    '''
    eltfreq = eltfreq.rename({'vid':'VOCAB'},axis=1)
    VOCAB_SIZE = int(eltfreq['VOCAB'].max()) + 1 ## for zero
    eltfreq.to_csv("forcallback.txt",sep="\t")
    NEW_DRUGS = newdrugs.shape[0]
    print("vocab_size = ", VOCAB_SIZE, " new drugs = ",NEW_DRUGS)
    os.environ["CUDA_VISIBLE_DEVICES"]=settings['cuda']



    step_per_ep = int(settings['totdat']/settings['batchsize']) # 159000
    val_step =  int(settings['valdat']/settings['batchsize']) #4300 #17700 #1000 #
    print(fname)
    #pdb.set_trace()
    print( "\n".join(settingsList))
    model_util.tf_sess()            
    model = Sequential()
    if 'resumefile' in settings:
        model = load_model(settings['resumefile']) 
    else:
        with tf.device('/gpu:0'):
            embedding = Embedding(VOCAB_SIZE, settings['embed_size'], name='embed1', input_length=1)
            model.add(embedding)
            model.add(Reshape((settings['embed_size'],)))
            dropoutLayer = Dropout(settings['dropout'])
            model.add(dropoutLayer)

            model.add(Dense(NEW_DRUGS, activation='softmax',
                            kernel_regularizer=regularizers.l2(settings['L2']))) #"softmax"))

            opt = model_util.get_opt(settings)
            model.compile(loss='categorical_crossentropy', optimizer =opt,metrics=['accuracy']) #'adam'
            ###########
            ###########

    #files = glob.glob('../04.03_eventemb/ez/skipdat.*')
    files = glob.glob(skips)
    if 'max_files' in settings:
        files = files[:settings['max_files']]
    test_files = np.random.choice(files, int(.1*len(files)))
    files = list(set(files) - set(test_files))
    gTrain = generate_generic(files,settings['batchsize'], NEW_DRUGS)
    gValid = generate_generic(test_files,settings['batchsize'], NEW_DRUGS)
    checkpoint = ModelCheckpoint(filepath=fname + "-{epoch:03d}.h5")

    print('verboseME')
    early = keras.callbacks.EarlyStopping(monitor='val_loss',
                              min_delta=0,
                              patience=2,
                              verbose=1, mode='auto')    
    time_callback = model_util.TimeHistory()
    val_callback = ValGo()
    callbacklist = [checkpoint, time_callback, val_callback]
    if do_early_stopping:
        callbacklist.append(early)
    history = model.fit_generator(generator=gTrain, verbose=1,
            validation_data = gValid, validation_steps=val_step,
                                  steps_per_epoch=step_per_ep, epochs=settings['numep'],
                                  callbacks=callbacklist) #, early, class_weight=, callbacks=[mycallback,early_stopping])
    model_util.end_save(model, history, fname, settingsList, time_callback)
    
    f = open(fname + ".val",'wb')
    pickle.dump((val_callback.valid_examples, val_callback.results),f)
    f.close()

    
    #pd.DataFrame(accuracy.history).to_csv(fname + ".accuracy.csv")
    #f = open(fname + ".history.pkl",'wb')
    #pickle.dump(history,f)
    #f.close()

def generate_generic(files, batch_size, num_classes,  onlyDx=False):
    #fdo = files[22]
    while True:
        #print("RESTART GEN!")
        np.random.shuffle(files)
        for fi in files:
            #print(fi)
            #fi = 'sample3/skipdat.8.32' ## OVERFIT #fdo #
            dat = pd.read_csv(fi,sep="\t",header=None).values
            i0 = 0
            #i1 = batch_size
            nrow = dat.shape[0]
            ix = np.arange(nrow)
            np.random.shuffle(ix)
            dat = dat[ix,:]
            #print(dat[:5,:])
            #print(max(dat[:,1]))
            labs = np_utils.to_categorical(dat[:,1], num_classes=num_classes)
            while i0 < nrow:
                rowend = min(i0 + batch_size,nrow)
                yield (dat[i0:rowend,0], labs[i0:rowend,:])
                # batch[:,1])
                i0 += batch_size

class ValGo(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.eltfreq = pd.read_csv("forcallback.txt",sep="\t")
        self.valid_examples = []
        for typ in [i for i in set(self.eltfreq['type']) if not pd.isnull(i)]:
            options = list(self.eltfreq.loc[self.eltfreq['type']==typ,:].sort_values('ct',ascending=False)[:100]['VOCAB'])
            #print(typ, options)
            self.valid_examples.extend(list(np.random.choice(options,4)))
        #self.valid_examples = list(np.random.choice(list(self.eltfreq.loc[~self.eltfreq['isrx'],:].sort_values('ct',ascending=False)[:100]['eltid'].map(int)),8)) 
        #if self.eltfreq.shape[0] > 635:
        #    self.valid_examples = self.valid_examples[:4] + list(np.random.choice(list(self.eltfreq.loc[self.eltfreq['isrx'],:].sort_values('ct',ascending=False)[:100]['eltid'].map(int)),4))
        self.valid_dataset = tf.constant(self.valid_examples, dtype=tf.int32)

        #emb = tf.Variable(self.model.get_layer("embed1").get_weights()[0])
        self.results = []
        
    def on_epoch_end(self, batch, logs={}):
        emb = tf.Variable(self.model.get_layer("embed1").get_weights()[0])
        norm = tf.sqrt(tf.reduce_sum(tf.square(emb), 1, keep_dims=True))
        normalized_embeddings = emb / norm
        valid_embeddings = tf.nn.embedding_lookup(
                normalized_embeddings, self.valid_dataset)
        self.similarity = tf.matmul(
                valid_embeddings, normalized_embeddings, transpose_b=True)
        
        sim =  K.eval(self.similarity)
        self.results.append(sim)
        print("\n________")
        for i, v in enumerate(self.valid_examples):
            vname = self.eltfreq.loc[self.eltfreq['VOCAB']==v,'code'].item()
            top_k = 6
            nearest = (-sim[i, :]).argsort()[1:top_k + 1]
            try:
                nnames = [self.eltfreq.loc[self.eltfreq['VOCAB']==k,'code'].item()
                      if (self.eltfreq['VOCAB']==k).sum()==1 else "EMPTY"
                      for k in nearest]
                #print("; ".join(nnames))
                print("Nearest to " + vname + ": " + "; ".join(nnames))
            except TypeError:
                print("Typerror!", vname)
                print(nearest)




            
import argparse
#bash run_emb.sh -l2 .001 -lr .0001 -es 50 -ep 198 -st 0 -skips skips-60-480-0.6  -max_files 2400 -voc vocab_drugs.pkl
if __name__ == "__main__":
    #huh()

    parser = argparse.ArgumentParser(description='Setup.')
    parser.add_argument('-l2', dest='l2', 
                     default=10**(-1*(np.random.rand(1)*3 + 1)[0]),
                        help='sum the integers (default: find the max)',type=float)
    parser.add_argument('-cuda', dest='cuda', 
                        default='0')
    parser.add_argument('-es', dest='embed_size', default=10,type=int)
    parser.add_argument('-lr', dest='learning_rate',
                        default=10**(-1*(np.random.rand(1)*4 + 1.5)[0]),type=float)
    parser.add_argument('-st', dest='earlystopping',
                        default=0,type=int)
    parser.add_argument('-ep', dest='numep',
                        default=20,type=int)
    parser.add_argument('-bs', dest='batchsize',
                        default=200,type=int)
    
    parser.add_argument('-resume', dest='resumefile',
                        default="NO",type=str)
    parser.add_argument('-skips', dest='skippath',
                        default='groupvoc/skipdat.*',type=str)
    parser.add_argument('-save', dest='savepath',
                        default='',type=str)
    parser.add_argument('-max_files', dest='max_files',type=int)
    
    voc = "../../data/clid.vi.allvocab.pkl"     # voc = "../../data/clid.uniqvoc.pkl"
    parser.add_argument('-voc', dest='vocab',type=str,default=voc)
    args = parser.parse_args()
    #learning_rate = sys.argv[3]
    prefix = "embgr/"


    settings = {'opt':'adam', 'L2':args.l2, 'cuda':'0','batchsize':args.batchsize,
                'numep':args.numep,'totdat':5000000,'valdat':3000000,
                'embed_size':args.embed_size}
    if 'max_files' in args:
        print("max_files = ", args.max_files)
        settings['max_files'] = args.max_files
    if args.learning_rate > 0:
        settings['learning_rate']  = args.learning_rate
    if not args.resumefile == "NO":
        settings['resumefile']  = args.resumefile
    saveto = args.savepath
    if not saveto:
        saveto = args.skippath.strip("/") + "-tf/"
    if not saveto.endswith("/"):
        saveto += "/"
    if not os.path.exists(saveto):
        os.mkdir(saveto)
    print(args)

    run(saveto,args.vocab, args.skippath + "/skipdat.*",
        do_early_stopping = args.earlystopping==True,
                settings=settings)




def get_dfs(fname, drugdf,embonly=True, epoch=-1):
    toopen = '{:s}-{:03d}.h5'.format(fname,epoch ) if epoch > 0 else '{:s}-{:s}.h5'.format(fname, 'best')
    if not os.path.exists(toopen):
        return None
    e0 = load_model(toopen) 
    #d0 = pd.DataFrame[0],columns=drugs)
    emb = pd.DataFrame(e0.get_layer('embed1').get_weights()[0])
    emb = (emb.transpose()/ ((emb**2).sum(axis=1)**.5 )).transpose() #.var(axis=1)

    #emb.index = ef['name']
    if embonly:
        return emb
    #ef = eltfreq_new.loc[eltfreq_new['name'].isin(nam),:].copy()
    #ef['eltid'] = ef['eltid']-3013
    #ef = ef.loc[ef['eltid'].isin(emb.index),:]

    #ef = ef.loc[ef['eltid'].isin(emb.index),:]
    #ev = pd.DataFrame(emb.loc[ef['eltid'],:])
    #ev.index = ef['name'] #eltfreq_new.loc[eltfreq_new['name'].isin(nam),'name']
    namp = e0.predict(np.arange(emb.shape[0]))
    if namp.shape[1] > drugdf.shape[0]:
        namp = namp[:,:drugdf.shape[0]]
    namp = pd.DataFrame(namp, columns = drugdf['name']) # #ef['eltid']))
    #namp.index = ef['name'] #eltfreq_new.loc[eltfreq_new['name'].isin(nam),'name']
    #namp.columns = drugdf['name']
    wt = e0.layers[3].get_weights()[0]
    wt = wt/((wt**2).sum(axis=0)**.5 )
    if wt.shape[1] > drugdf.shape[0]:
        print("trim WT",wt.shape[1],drugdf.shape[0])
        wt = wt[:,:drugdf.shape[0]]
    d0 = pd.DataFrame(wt,columns=drugdf['name'])
    return emb, d0, namp
